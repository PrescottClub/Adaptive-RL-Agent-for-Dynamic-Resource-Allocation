import numpy as np
import gymnasium as gym
from gymnasium import spaces
from typing import Dict, List, Tuple, Any
import random
from dataclasses import dataclass
from enum import Enum

class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    NETWORK_TRAFFIC = "network_traffic"
    CLOUD_COMPUTING = "cloud_computing"
    SMART_GRID = "smart_grid"
    VEHICLE_ROUTING = "vehicle_routing"

@dataclass
class TaskConfig:
    """ä»»åŠ¡é…ç½®ç±»"""
    task_type: TaskType
    resource_count: int
    demand_pattern: str
    constraint_type: str
    reward_weights: Dict[str, float]
    difficulty_level: float
    
class MetaTaskGenerator:
    """
    å…ƒå­¦ä¹ ä»»åŠ¡ç”Ÿæˆå™¨ - è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„èµ„æºåˆ†é…åœºæ™¯
    
    è¿™æ˜¯å…ƒå­¦ä¹ ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œèƒ½å¤Ÿï¼š
    1. ç”Ÿæˆä¸åŒé¢†åŸŸçš„èµ„æºåˆ†é…ä»»åŠ¡
    2. æ§åˆ¶ä»»åŠ¡éš¾åº¦å’Œå¤šæ ·æ€§
    3. ç¡®ä¿ä»»åŠ¡é—´çš„ç›¸å…³æ€§å’Œå·®å¼‚æ€§
    """
    
    def __init__(self, seed=None):
        self.rng = np.random.RandomState(seed)
        self.task_templates = self._initialize_task_templates()
        
    def _initialize_task_templates(self) -> Dict[TaskType, Dict]:
        """åˆå§‹åŒ–ä»»åŠ¡æ¨¡æ¿"""
        return {
            TaskType.NETWORK_TRAFFIC: {
                'base_resources': 4,  # è§†é¢‘ã€æ¸¸æˆã€ä¸‹è½½ã€æµè§ˆ
                'demand_patterns': ['peak_hours', 'random_burst', 'gradual_increase'],
                'constraints': ['bandwidth_limit', 'latency_sensitive', 'fair_share'],
                'reward_components': ['throughput', 'latency', 'fairness', 'utilization']
            },
            TaskType.CLOUD_COMPUTING: {
                'base_resources': 3,  # CPUã€å†…å­˜ã€å­˜å‚¨
                'demand_patterns': ['workload_spike', 'periodic_batch', 'steady_growth'],
                'constraints': ['cost_budget', 'sla_guarantee', 'energy_limit'],
                'reward_components': ['performance', 'cost', 'sla_compliance', 'energy_efficiency']
            },
            TaskType.SMART_GRID: {
                'base_resources': 5,  # å¤ªé˜³èƒ½ã€é£èƒ½ã€ç«ç”µã€æ°´ç”µã€å‚¨èƒ½
                'demand_patterns': ['daily_cycle', 'weather_dependent', 'industrial_load'],
                'constraints': ['carbon_limit', 'stability_requirement', 'cost_optimization'],
                'reward_components': ['stability', 'carbon_footprint', 'cost', 'renewable_ratio']
            },
            TaskType.VEHICLE_ROUTING: {
                'base_resources': 6,  # ä¸åŒç±»å‹è½¦è¾†å’Œè·¯çº¿
                'demand_patterns': ['rush_hour', 'event_driven', 'weather_impact'],
                'constraints': ['fuel_limit', 'time_window', 'capacity_constraint'],
                'reward_components': ['travel_time', 'fuel_consumption', 'customer_satisfaction', 'vehicle_utilization']
            }
        }
    
    def generate_task(self, task_type: TaskType = None, difficulty: float = None) -> TaskConfig:
        """
        ç”Ÿæˆä¸€ä¸ªæ–°çš„ä»»åŠ¡é…ç½®
        
        Args:
            task_type: æŒ‡å®šä»»åŠ¡ç±»å‹ï¼ŒNoneåˆ™éšæœºé€‰æ‹©
            difficulty: ä»»åŠ¡éš¾åº¦ (0.0-1.0)ï¼ŒNoneåˆ™éšæœºé€‰æ‹©
            
        Returns:
            TaskConfig: ç”Ÿæˆçš„ä»»åŠ¡é…ç½®
        """
        if task_type is None:
            task_type = self.rng.choice(list(TaskType))
        
        if difficulty is None:
            difficulty = self.rng.uniform(0.2, 0.9)
        
        template = self.task_templates[task_type]
        
        # æ ¹æ®éš¾åº¦è°ƒæ•´èµ„æºæ•°é‡
        base_count = template['base_resources']
        resource_count = base_count + int(difficulty * 4)  # æœ€å¤šå¢åŠ 4ä¸ªèµ„æº
        
        # éšæœºé€‰æ‹©éœ€æ±‚æ¨¡å¼å’Œçº¦æŸ
        demand_pattern = self.rng.choice(template['demand_patterns'])
        constraint_type = self.rng.choice(template['constraints'])
        
        # ç”Ÿæˆå¥–åŠ±æƒé‡
        reward_weights = self._generate_reward_weights(template['reward_components'], difficulty)
        
        return TaskConfig(
            task_type=task_type,
            resource_count=resource_count,
            demand_pattern=demand_pattern,
            constraint_type=constraint_type,
            reward_weights=reward_weights,
            difficulty_level=difficulty
        )
    
    def _generate_reward_weights(self, components: List[str], difficulty: float) -> Dict[str, float]:
        """ç”Ÿæˆå¥–åŠ±æƒé‡"""
        weights = {}
        
        # åŸºç¡€æƒé‡
        base_weights = self.rng.dirichlet([1.0] * len(components))
        
        # æ ¹æ®éš¾åº¦è°ƒæ•´æƒé‡åˆ†å¸ƒ
        if difficulty > 0.7:
            # é«˜éš¾åº¦ï¼šæƒé‡æ›´ä¸å‡åŒ€ï¼Œå¢åŠ æŒ‘æˆ˜
            concentration = 0.5
        else:
            # ä½éš¾åº¦ï¼šæƒé‡ç›¸å¯¹å‡åŒ€
            concentration = 2.0
        
        adjusted_weights = self.rng.dirichlet([concentration] * len(components))
        
        for i, component in enumerate(components):
            weights[component] = float(adjusted_weights[i])
        
        return weights
    
    def generate_task_batch(self, batch_size: int, 
                           task_distribution: Dict[TaskType, float] = None) -> List[TaskConfig]:
        """
        ç”Ÿæˆä¸€æ‰¹ä»»åŠ¡
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            task_distribution: ä»»åŠ¡ç±»å‹åˆ†å¸ƒï¼ŒNoneåˆ™å‡åŒ€åˆ†å¸ƒ
            
        Returns:
            List[TaskConfig]: ä»»åŠ¡é…ç½®åˆ—è¡¨
        """
        if task_distribution is None:
            task_distribution = {task_type: 1.0 for task_type in TaskType}
        
        # å½’ä¸€åŒ–åˆ†å¸ƒ
        total_weight = sum(task_distribution.values())
        normalized_dist = {k: v/total_weight for k, v in task_distribution.items()}
        
        tasks = []
        for _ in range(batch_size):
            # æ ¹æ®åˆ†å¸ƒé€‰æ‹©ä»»åŠ¡ç±»å‹
            rand_val = self.rng.random()
            cumulative = 0.0
            selected_type = list(TaskType)[0]  # é»˜è®¤å€¼
            
            for task_type, weight in normalized_dist.items():
                cumulative += weight
                if rand_val <= cumulative:
                    selected_type = task_type
                    break
            
            task = self.generate_task(task_type=selected_type)
            tasks.append(task)
        
        return tasks
    
    def create_curriculum(self, total_tasks: int, 
                         difficulty_progression: str = 'linear') -> List[TaskConfig]:
        """
        åˆ›å»ºè¯¾ç¨‹å­¦ä¹ åºåˆ—
        
        Args:
            total_tasks: æ€»ä»»åŠ¡æ•°
            difficulty_progression: éš¾åº¦é€’å¢æ–¹å¼ ('linear', 'exponential', 'step')
            
        Returns:
            List[TaskConfig]: æŒ‰éš¾åº¦æ’åºçš„ä»»åŠ¡åºåˆ—
        """
        tasks = []
        
        for i in range(total_tasks):
            progress = i / (total_tasks - 1)
            
            if difficulty_progression == 'linear':
                difficulty = 0.1 + 0.8 * progress
            elif difficulty_progression == 'exponential':
                difficulty = 0.1 + 0.8 * (progress ** 2)
            elif difficulty_progression == 'step':
                difficulty = 0.1 + 0.8 * (int(progress * 4) / 4)
            else:
                difficulty = 0.5  # é»˜è®¤ä¸­ç­‰éš¾åº¦
            
            task = self.generate_task(difficulty=difficulty)
            tasks.append(task)
        
        return tasks

class MetaEnvironmentWrapper(gym.Env):
    """
    å…ƒç¯å¢ƒåŒ…è£…å™¨ - å°†ä»»åŠ¡é…ç½®è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„ç¯å¢ƒ
    """
    
    def __init__(self, base_env_class, task_config: TaskConfig):
        super().__init__()
        self.base_env_class = base_env_class
        self.task_config = task_config
        self.base_env = None
        self._setup_environment()
    
    def _setup_environment(self):
        """æ ¹æ®ä»»åŠ¡é…ç½®è®¾ç½®ç¯å¢ƒ"""
        # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
        self.base_env = self.base_env_class()
        
        # æ ¹æ®ä»»åŠ¡é…ç½®è°ƒæ•´ç¯å¢ƒå‚æ•°
        self._adapt_observation_space()
        self._adapt_action_space()
        self._adapt_reward_function()
    
    def _adapt_observation_space(self):
        """é€‚é…è§‚å¯Ÿç©ºé—´"""
        # æ ¹æ®èµ„æºæ•°é‡è°ƒæ•´è§‚å¯Ÿç©ºé—´
        obs_dim = self.task_config.resource_count * 2  # éœ€æ±‚ + åˆ†é…
        self.observation_space = spaces.Box(
            low=0, high=1, 
            shape=(obs_dim,), 
            dtype=np.float32
        )
    
    def _adapt_action_space(self):
        """é€‚é…åŠ¨ä½œç©ºé—´"""
        # åŠ¨ä½œæ•°é‡ = èµ„æºæ•°é‡ * 2 (å¢åŠ /å‡å°‘)
        action_count = self.task_config.resource_count * 2
        self.action_space = spaces.Discrete(action_count)
    
    def _adapt_reward_function(self):
        """é€‚é…å¥–åŠ±å‡½æ•°"""
        self.reward_weights = self.task_config.reward_weights
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        if self.base_env is None:
            self._setup_environment()
        
        obs, info = self.base_env.reset(seed=seed, options=options)
        
        # æ ¹æ®ä»»åŠ¡é…ç½®è°ƒæ•´åˆå§‹çŠ¶æ€
        adapted_obs = self._adapt_observation(obs)
        adapted_info = self._adapt_info(info)
        
        return adapted_obs, adapted_info
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # å°†å…ƒåŠ¨ä½œè½¬æ¢ä¸ºåŸºç¡€ç¯å¢ƒåŠ¨ä½œ
        base_action = self._adapt_action(action)
        
        obs, reward, done, truncated, info = self.base_env.step(base_action)
        
        # é€‚é…è¾“å‡º
        adapted_obs = self._adapt_observation(obs)
        adapted_reward = self._adapt_reward(reward, obs, action)
        adapted_info = self._adapt_info(info)
        
        return adapted_obs, adapted_reward, done, truncated, adapted_info
    
    def _adapt_observation(self, obs):
        """é€‚é…è§‚å¯Ÿå€¼"""
        # ç®€å•å®ç°ï¼šæˆªæ–­æˆ–å¡«å……åˆ°ç›®æ ‡ç»´åº¦
        target_dim = self.observation_space.shape[0]
        if len(obs) > target_dim:
            return obs[:target_dim]
        elif len(obs) < target_dim:
            padded = np.zeros(target_dim)
            padded[:len(obs)] = obs
            return padded
        return obs
    
    def _adapt_action(self, action):
        """é€‚é…åŠ¨ä½œ"""
        # å°†å…ƒåŠ¨ä½œæ˜ å°„åˆ°åŸºç¡€ç¯å¢ƒåŠ¨ä½œ
        base_action_count = self.base_env.action_space.n
        return action % base_action_count
    
    def _adapt_reward(self, base_reward, obs, action):
        """é€‚é…å¥–åŠ±"""
        # æ ¹æ®ä»»åŠ¡é…ç½®çš„æƒé‡è°ƒæ•´å¥–åŠ±
        # è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ ¹æ®å…·ä½“ä»»åŠ¡ç±»å‹è®¡ç®—
        return base_reward * self.task_config.difficulty_level
    
    def _adapt_info(self, info):
        """é€‚é…ä¿¡æ¯"""
        info['task_config'] = self.task_config
        return info

if __name__ == '__main__':
    # æµ‹è¯•å…ƒä»»åŠ¡ç”Ÿæˆå™¨
    generator = MetaTaskGenerator(seed=42)
    
    print("ğŸ§ª æµ‹è¯•å…ƒä»»åŠ¡ç”Ÿæˆå™¨")
    
    # ç”Ÿæˆå•ä¸ªä»»åŠ¡
    task = generator.generate_task()
    print(f"\nğŸ“‹ ç”Ÿæˆçš„ä»»åŠ¡:")
    print(f"   ç±»å‹: {task.task_type.value}")
    print(f"   èµ„æºæ•°é‡: {task.resource_count}")
    print(f"   éœ€æ±‚æ¨¡å¼: {task.demand_pattern}")
    print(f"   çº¦æŸç±»å‹: {task.constraint_type}")
    print(f"   éš¾åº¦: {task.difficulty_level:.2f}")
    print(f"   å¥–åŠ±æƒé‡: {task.reward_weights}")
    
    # ç”Ÿæˆä»»åŠ¡æ‰¹æ¬¡
    batch = generator.generate_task_batch(5)
    print(f"\nğŸ“¦ ç”Ÿæˆçš„ä»»åŠ¡æ‰¹æ¬¡ (5ä¸ª):")
    for i, task in enumerate(batch):
        print(f"   {i+1}. {task.task_type.value} (éš¾åº¦: {task.difficulty_level:.2f})")
    
    # ç”Ÿæˆè¯¾ç¨‹å­¦ä¹ åºåˆ—
    curriculum = generator.create_curriculum(10, 'linear')
    print(f"\nğŸ“š è¯¾ç¨‹å­¦ä¹ åºåˆ— (10ä¸ªä»»åŠ¡):")
    for i, task in enumerate(curriculum):
        print(f"   {i+1}. {task.task_type.value} (éš¾åº¦: {task.difficulty_level:.2f})")
    
    print("\nâœ… å…ƒä»»åŠ¡ç”Ÿæˆå™¨æµ‹è¯•å®Œæˆï¼")

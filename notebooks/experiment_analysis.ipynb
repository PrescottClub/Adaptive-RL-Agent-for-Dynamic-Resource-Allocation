{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Adaptive RL Agent 实验分析\n\n",
        "这个 notebook 用于分析 DQN 和 Double DQN 在动态资源分配任务中的性能。\n\n",
        "## 📋 实验内容\n\n",
        "- 🌐 环境测试和分析\n",
        "- 🤖 智能体训练\n",
        "- 📊 性能对比\n",
        "- 📈 结果可视化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置路径\n",
        "current_dir = Path().resolve()\n",
        "project_root = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f'📁 项目根目录: {project_root}')\n",
        "print(f'📁 当前目录: {current_dir}')\n",
        "print(f'🔥 PyTorch 版本: {torch.__version__}')\n",
        "print(f'💻 设备: {\"CUDA\" if torch.cuda.is_available() else \"CPU\"}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入自定义模块\n",
        "try:\n",
        "    from src.environments.network_traffic_env import DynamicTrafficEnv\n",
        "    from src.agents.dqn_agent import DQNAgent\n",
        "    from src.agents.double_dqn_agent import DoubleDQNAgent\n",
        "    print('✅ 所有模块导入成功！')\n",
        "except ImportError as e:\n",
        "    print(f'❌ 导入错误: {e}')\n",
        "    print('请确保在项目根目录运行此 notebook')\n",
        "    # 尝试备用导入方式\n",
        "    sys.path.append(str(project_root / 'src'))\n",
        "    try:\n",
        "        from environments.network_traffic_env import DynamicTrafficEnv\n",
        "        from agents.dqn_agent import DQNAgent\n",
        "        from agents.double_dqn_agent import DoubleDQNAgent\n",
        "        print('✅ 备用导入方式成功！')\n",
        "    except ImportError as e2:\n",
        "        print(f'❌ 备用导入也失败: {e2}')\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建和测试环境\n",
        "print('🌐 创建环境...')\n",
        "env = DynamicTrafficEnv(render_mode=None)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print(f'📏 状态空间维度: {state_size}')\n",
        "print(f'🎮 动作空间大小: {action_size}')\n",
        "print(f'⏱️ 最大步数: {env.max_steps}')\n",
        "\n",
        "# 测试环境\n",
        "state, info = env.reset()\n",
        "print(f'\\n🧪 环境测试:')\n",
        "print(f'  初始状态: {state}')\n",
        "action = env.action_space.sample()\n",
        "next_state, reward, done, truncated, info = env.step(action)\n",
        "print(f'  随机动作 {action} -> 奖励: {reward:.3f}')\n",
        "print('✅ 环境创建和测试成功！')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 智能体配置和训练\n\n",
        "现在配置和训练 DQN 和 Double DQN 智能体。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 智能体配置 (使用正确的参数名称)\n",
        "print(f'💻 设备: {\"CUDA\" if torch.cuda.is_available() else \"CPU\"}')\n",
        "\n",
        "# 正确的 DQNAgent 参数\n",
        "AGENT_CONFIG = {\n",
        "    'state_size': state_size,\n",
        "    'action_size': action_size,\n",
        "    'lr': 5e-4,\n",
        "    'buffer_size': 10000,\n",
        "    'batch_size': 64,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 1e-3,\n",
        "    'update_every': 4,\n",
        "    'epsilon': 1.0,\n",
        "    'epsilon_min': 0.01,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "print('🤖 智能体配置:')\n",
        "for key, value in AGENT_CONFIG.items():\n",
        "    print(f'  {key}: {value}')\n",
        "\n",
        "# 创建智能体\n",
        "print('\\n🔧 创建智能体...')\n",
        "dqn_agent = DQNAgent(**AGENT_CONFIG)\n",
        "ddqn_agent = DoubleDQNAgent(**AGENT_CONFIG)\n",
        "print('✅ DQN 和 Double DQN 智能体创建完成！')\n",
        "print(f'📱 DQN 设备: {dqn_agent.device}')\n",
        "print(f'📱 Double DQN 设备: {ddqn_agent.device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练函数\n",
        "def train_agent(agent, agent_name, episodes=100):\n",
        "    \"\"\"训练智能体并返回分数历史\"\"\"\n",
        "    print(f'🚀 开始训练 {agent_name}...')\n",
        "    scores = []\n",
        "    \n",
        "    for episode in range(1, episodes + 1):\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "        \n",
        "        for step in range(100):  # 每回合最多100步\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            \n",
        "            state = next_state\n",
        "            score += reward\n",
        "            \n",
        "            if done or truncated:\n",
        "                break\n",
        "        \n",
        "        scores.append(score)\n",
        "        \n",
        "        # 手动更新epsilon (因为DQNAgent在learn中自动更新)\n",
        "        if hasattr(agent, 'epsilon') and agent.epsilon > AGENT_CONFIG['epsilon_min']:\n",
        "            agent.epsilon = max(AGENT_CONFIG['epsilon_min'], \n",
        "                               agent.epsilon * AGENT_CONFIG['epsilon_decay'])\n",
        "        \n",
        "        if episode % 25 == 0:\n",
        "            avg_score = np.mean(scores[-25:])\n",
        "            current_eps = getattr(agent, 'epsilon', 0.0)\n",
        "            print(f'  回合 {episode:3d} | 平均分数: {avg_score:7.2f} | ε: {current_eps:.3f}')\n",
        "    \n",
        "    final_avg = np.mean(scores[-25:])\n",
        "    print(f'✅ {agent_name} 训练完成! 最终平均分数: {final_avg:.2f}')\n",
        "    return scores\n",
        "\n",
        "print('🛠️ 训练函数准备就绪！')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练 DQN\n",
        "print('=' * 50)\n",
        "print('🤖 训练 DQN 智能体')\n",
        "print('=' * 50)\n",
        "dqn_scores = train_agent(dqn_agent, 'DQN', episodes=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练 Double DQN\n",
        "print('\\n' + '=' * 50)\n",
        "print('🤖 训练 Double DQN 智能体')\n",
        "print('=' * 50)\n",
        "ddqn_scores = train_agent(ddqn_agent, 'Double DQN', episodes=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 性能分析和可视化\n\n",
        "比较两个智能体的训练性能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 绘制训练曲线\n",
        "def plot_training_curves(dqn_scores, ddqn_scores):\n",
        "    \"\"\"绘制训练曲线对比图\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # 计算移动平均\n",
        "    def moving_average(scores, window=25):\n",
        "        return [np.mean(scores[max(0, i-window):i+1]) for i in range(len(scores))]\n",
        "    \n",
        "    dqn_ma = moving_average(dqn_scores)\n",
        "    ddqn_ma = moving_average(ddqn_scores)\n",
        "    \n",
        "    # 子图1: 原始分数\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(dqn_scores, alpha=0.3, color='blue', label='DQN 原始分数')\n",
        "    plt.plot(ddqn_scores, alpha=0.3, color='red', label='Double DQN 原始分数')\n",
        "    plt.plot(dqn_ma, color='blue', linewidth=2, label='DQN 移动平均')\n",
        "    plt.plot(ddqn_ma, color='red', linewidth=2, label='Double DQN 移动平均')\n",
        "    plt.xlabel('回合')\n",
        "    plt.ylabel('分数')\n",
        "    plt.title('🏆 训练分数对比')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图2: 移动平均对比\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(dqn_ma, color='blue', linewidth=2, label='DQN')\n",
        "    plt.plot(ddqn_ma, color='red', linewidth=2, label='Double DQN')\n",
        "    plt.xlabel('回合')\n",
        "    plt.ylabel('移动平均分数')\n",
        "    plt.title('📈 移动平均分数对比 (窗口=25)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图3: 分数分布\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.hist(dqn_scores, bins=20, alpha=0.7, color='blue', label='DQN', density=True)\n",
        "    plt.hist(ddqn_scores, bins=20, alpha=0.7, color='red', label='Double DQN', density=True)\n",
        "    plt.xlabel('分数')\n",
        "    plt.ylabel('密度')\n",
        "    plt.title('📊 分数分布对比')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图4: 性能统计\n",
        "    plt.subplot(2, 2, 4)\n",
        "    stats_data = {\n",
        "        'DQN': [np.mean(dqn_scores), np.std(dqn_scores), np.max(dqn_scores), np.min(dqn_scores)],\n",
        "        'Double DQN': [np.mean(ddqn_scores), np.std(ddqn_scores), np.max(ddqn_scores), np.min(ddqn_scores)]\n",
        "    }\n",
        "    \n",
        "    x = np.arange(4)\n",
        "    width = 0.35\n",
        "    \n",
        "    plt.bar(x - width/2, stats_data['DQN'], width, label='DQN', color='blue', alpha=0.7)\n",
        "    plt.bar(x + width/2, stats_data['Double DQN'], width, label='Double DQN', color='red', alpha=0.7)\n",
        "    \n",
        "    plt.xlabel('统计指标')\n",
        "    plt.ylabel('值')\n",
        "    plt.title('📈 性能统计对比')\n",
        "    plt.xticks(x, ['平均值', '标准差', '最大值', '最小值'])\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 绘制图表\n",
        "plot_training_curves(dqn_scores, ddqn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 实验总结\n\n",
        "### 主要发现:\n\n",
        "1. **训练性能**: 比较了DQN和Double DQN在动态资源分配任务中的学习效率\n",
        "2. **稳定性**: 分析了两种算法的训练稳定性和收敛特性\n",
        "3. **最终性能**: 评估了训练完成后的智能体在测试环境中的表现\n\n",
        "### 技术要点:\n\n",
        "- **环境**: 8维状态空间，5个离散动作\n",
        "- **网络架构**: 3层全连接神经网络\n",
        "- **训练参数**: 学习率5e-4，经验回放缓冲区10K，批大小64\n",
        "- **探索策略**: ε-贪婪，从1.0衰减到0.01\n\n",
        "### 应用价值:\n\n",
        "这个实验展示了深度强化学习在网络资源分配中的应用潜力，为实际的动态资源管理系统提供了技术基础。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
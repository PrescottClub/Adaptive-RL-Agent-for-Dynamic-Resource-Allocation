import numpy as np
import matplotlib.pyplot as plt
from collections import deque, defaultdict
import time
import json
from pathlib import Path
from typing import List, Dict, Tuple, Any
import torch

try:
    from ..environments.meta_task_generator import MetaTaskGenerator, MetaEnvironmentWrapper, TaskConfig
    from ..environments.network_traffic_env import DynamicTrafficEnv
    from ..agents.meta_dqn_agent import MetaDQNAgent
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    from pathlib import Path
    project_root = Path(__file__).parent.parent.parent
    sys.path.append(str(project_root))
    from src.environments.meta_task_generator import MetaTaskGenerator, MetaEnvironmentWrapper, TaskConfig
    from src.environments.network_traffic_env import DynamicTrafficEnv
    from src.agents.meta_dqn_agent import MetaDQNAgent

class MetaTrainer:
    """
    å…ƒå­¦ä¹ è®­ç»ƒå™¨ - å®ç°MAMLç®—æ³•çš„å®Œæ•´è®­ç»ƒæµç¨‹
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å…ƒè®­ç»ƒå¾ªç¯ï¼ˆåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå­¦ä¹ å¦‚ä½•å¿«é€Ÿå­¦ä¹ ï¼‰
    2. å¿«é€Ÿé€‚åº”æµ‹è¯•ï¼ˆéªŒè¯åœ¨æ–°ä»»åŠ¡ä¸Šçš„é€‚åº”èƒ½åŠ›ï¼‰
    3. è·¨åŸŸè¿ç§»è¯„ä¼°ï¼ˆæµ‹è¯•ä¸åŒé¢†åŸŸé—´çš„çŸ¥è¯†è¿ç§»ï¼‰
    4. æ€§èƒ½ç›‘æ§å’Œå¯è§†åŒ–
    """
    
    def __init__(self, agent: MetaDQNAgent, task_generator: MetaTaskGenerator, 
                 base_env_class=DynamicTrafficEnv, config=None):
        self.agent = agent
        self.task_generator = task_generator
        self.base_env_class = base_env_class
        self.config = config or self._default_config()
        
        # è®­ç»ƒç»Ÿè®¡
        self.meta_losses = []
        self.adaptation_scores = defaultdict(list)
        self.transfer_scores = defaultdict(list)
        self.task_performance = defaultdict(list)
        
        # æ—¶é—´ç»Ÿè®¡
        self.training_start_time = None
        self.episode_times = deque(maxlen=100)
        
    def _default_config(self):
        """é»˜è®¤é…ç½®"""
        return {
            'meta_episodes': 1000,
            'episodes_per_task': 50,
            'adaptation_episodes': 10,
            'meta_batch_size': 4,
            'support_size': 20,
            'query_size': 10,
            'max_steps_per_episode': 200,
            'eval_frequency': 50,
            'save_frequency': 100,
            'early_stopping_patience': 200,
            'target_adaptation_score': 150.0
        }
    
    def meta_train(self, save_path='models/meta_dqn'):
        """
        å…ƒè®­ç»ƒä¸»å¾ªç¯
        
        Args:
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        self.training_start_time = time.time()
        print("ğŸš€ å¼€å§‹å…ƒå­¦ä¹ è®­ç»ƒ...")
        print(f"ğŸ“Š é…ç½®: {self.config}")
        
        best_adaptation_score = float('-inf')
        patience_counter = 0
        
        for meta_episode in range(1, self.config['meta_episodes'] + 1):
            episode_start_time = time.time()
            
            # ç”Ÿæˆå…ƒæ‰¹æ¬¡ä»»åŠ¡
            meta_batch_tasks = self.task_generator.generate_task_batch(
                self.config['meta_batch_size']
            )
            
            # æ”¶é›†å…ƒæ‰¹æ¬¡æ•°æ®
            meta_batch_data = []
            for task_config in meta_batch_tasks:
                task_data = self._collect_task_data(task_config)
                meta_batch_data.append(task_data)
            
            # æ‰§è¡Œå…ƒæ›´æ–°
            meta_loss = self.agent.meta_update(meta_batch_data)
            self.meta_losses.append(meta_loss)
            
            # è®°å½•æ—¶é—´
            episode_time = time.time() - episode_start_time
            self.episode_times.append(episode_time)
            
            # å®šæœŸè¯„ä¼°
            if meta_episode % self.config['eval_frequency'] == 0:
                adaptation_score = self._evaluate_adaptation()
                transfer_score = self._evaluate_transfer()
                
                print(f"ğŸ“Š å…ƒå›åˆ {meta_episode:4d} | "
                      f"å…ƒæŸå¤±: {meta_loss:.4f} | "
                      f"é€‚åº”åˆ†æ•°: {adaptation_score:.2f} | "
                      f"è¿ç§»åˆ†æ•°: {transfer_score:.2f} | "
                      f"ç”¨æ—¶: {episode_time:.2f}s")
                
                # æ—©åœæ£€æŸ¥
                if adaptation_score > best_adaptation_score:
                    best_adaptation_score = adaptation_score
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    self.agent.save(f"{save_path}_best.pth")
                else:
                    patience_counter += 1
                
                if patience_counter >= self.config['early_stopping_patience']:
                    print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œæœ€ä½³é€‚åº”åˆ†æ•°: {best_adaptation_score:.2f}")
                    break
            
            # å®šæœŸä¿å­˜
            if meta_episode % self.config['save_frequency'] == 0:
                self.agent.save(f"{save_path}_episode_{meta_episode}.pth")
                self._save_training_log(f"{save_path}_log.json")
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - self.training_start_time
        print(f"ğŸ‰ å…ƒè®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/60:.2f} åˆ†é’Ÿ")
        print(f"ğŸ† æœ€ä½³é€‚åº”åˆ†æ•°: {best_adaptation_score:.2f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.agent.save(f"{save_path}_final.pth")
        self._save_training_log(f"{save_path}_final_log.json")
        
        return self.meta_losses, best_adaptation_score
    
    def _collect_task_data(self, task_config: TaskConfig) -> Dict:
        """
        ä¸ºå•ä¸ªä»»åŠ¡æ”¶é›†æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†æ•°æ®
        
        Args:
            task_config: ä»»åŠ¡é…ç½®
            
        Returns:
            Dict: åŒ…å«æ”¯æŒé›†ã€æŸ¥è¯¢é›†å’Œä»»åŠ¡é…ç½®çš„å­—å…¸
        """
        # åˆ›å»ºä»»åŠ¡ç¯å¢ƒ
        env = MetaEnvironmentWrapper(self.base_env_class, task_config)
        
        # è®¾ç½®æ™ºèƒ½ä½“ä»»åŠ¡
        self.agent.set_task(task_config)
        
        support_data = []
        query_data = []
        
        # æ”¶é›†æ”¯æŒé›†æ•°æ®
        for _ in range(self.config['support_size']):
            state, _ = env.reset()
            action = self.agent.act(state, eps=0.3)  # è¾ƒé«˜çš„æ¢ç´¢ç‡
            next_state, reward, done, truncated, _ = env.step(action)
            support_data.append((state, action, reward, next_state, done or truncated))
        
        # åœ¨æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚åº”
        self.agent.fast_adapt(support_data)
        
        # æ”¶é›†æŸ¥è¯¢é›†æ•°æ®
        for _ in range(self.config['query_size']):
            state, _ = env.reset()
            action = self.agent.act(state, eps=0.1)  # è¾ƒä½çš„æ¢ç´¢ç‡
            next_state, reward, done, truncated, _ = env.step(action)
            query_data.append((state, action, reward, next_state, done or truncated))
        
        return {
            'support': support_data,
            'query': query_data,
            'task_config': task_config
        }
    
    def _evaluate_adaptation(self) -> float:
        """
        è¯„ä¼°å¿«é€Ÿé€‚åº”èƒ½åŠ›
        
        Returns:
            float: é€‚åº”åˆ†æ•°ï¼ˆå¹³å‡å›åˆå¥–åŠ±ï¼‰
        """
        # ç”Ÿæˆæ–°çš„æµ‹è¯•ä»»åŠ¡
        test_tasks = self.task_generator.generate_task_batch(5)
        adaptation_scores = []
        
        for task_config in test_tasks:
            env = MetaEnvironmentWrapper(self.base_env_class, task_config)
            self.agent.set_task(task_config)
            
            # æ”¶é›†å°‘é‡æ”¯æŒæ•°æ®
            support_data = []
            for _ in range(10):  # åªç”¨10ä¸ªæ ·æœ¬è¿›è¡Œé€‚åº”
                state, _ = env.reset()
                action = self.agent.act(state, eps=0.2)
                next_state, reward, done, truncated, _ = env.step(action)
                support_data.append((state, action, reward, next_state, done or truncated))
            
            # å¿«é€Ÿé€‚åº”
            self.agent.fast_adapt(support_data)
            
            # æµ‹è¯•é€‚åº”åçš„æ€§èƒ½
            episode_scores = []
            for _ in range(5):  # æµ‹è¯•5ä¸ªå›åˆ
                state, _ = env.reset()
                total_reward = 0
                
                for step in range(self.config['max_steps_per_episode']):
                    action = self.agent.act(state, eps=0.0)  # è´ªå¿ƒç­–ç•¥
                    next_state, reward, done, truncated, _ = env.step(action)
                    total_reward += reward
                    state = next_state
                    
                    if done or truncated:
                        break
                
                episode_scores.append(total_reward)
            
            task_avg_score = np.mean(episode_scores)
            adaptation_scores.append(task_avg_score)
            self.adaptation_scores[task_config.task_type.value].append(task_avg_score)
        
        overall_adaptation_score = np.mean(adaptation_scores)
        return overall_adaptation_score
    
    def _evaluate_transfer(self) -> float:
        """
        è¯„ä¼°è·¨åŸŸè¿ç§»èƒ½åŠ›
        
        Returns:
            float: è¿ç§»åˆ†æ•°
        """
        # æµ‹è¯•ä¸åŒä»»åŠ¡ç±»å‹é—´çš„è¿ç§»
        try:
            from ..environments.meta_task_generator import TaskType
        except ImportError:
            from src.environments.meta_task_generator import TaskType
        
        task_types = list(TaskType)
        transfer_scores = []
        
        for source_type in task_types:
            for target_type in task_types:
                if source_type == target_type:
                    continue
                
                # åœ¨æºä»»åŠ¡ä¸Šè®­ç»ƒ
                source_task = self.task_generator.generate_task(source_type, difficulty=0.5)
                source_env = MetaEnvironmentWrapper(self.base_env_class, source_task)
                self.agent.set_task(source_task)
                
                # æ”¶é›†æºä»»åŠ¡æ•°æ®å¹¶é€‚åº”
                source_data = []
                for _ in range(20):
                    state, _ = source_env.reset()
                    action = self.agent.act(state, eps=0.2)
                    next_state, reward, done, truncated, _ = source_env.step(action)
                    source_data.append((state, action, reward, next_state, done or truncated))
                
                self.agent.fast_adapt(source_data)
                
                # åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šæµ‹è¯•ï¼ˆé›¶æ ·æœ¬è¿ç§»ï¼‰
                target_task = self.task_generator.generate_task(target_type, difficulty=0.5)
                target_env = MetaEnvironmentWrapper(self.base_env_class, target_task)
                self.agent.set_task(target_task)
                
                # æµ‹è¯•è¿ç§»æ€§èƒ½
                state, _ = target_env.reset()
                total_reward = 0
                
                for step in range(self.config['max_steps_per_episode']):
                    action = self.agent.act(state, eps=0.0)
                    next_state, reward, done, truncated, _ = target_env.step(action)
                    total_reward += reward
                    state = next_state
                    
                    if done or truncated:
                        break
                
                transfer_scores.append(total_reward)
                self.transfer_scores[f"{source_type.value}->{target_type.value}"].append(total_reward)
        
        overall_transfer_score = np.mean(transfer_scores) if transfer_scores else 0.0
        return overall_transfer_score
    
    def _save_training_log(self, filepath):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        log_data = {
            'meta_losses': self.meta_losses,
            'adaptation_scores': dict(self.adaptation_scores),
            'transfer_scores': dict(self.transfer_scores),
            'task_performance': dict(self.task_performance),
            'config': self.config,
            'training_time': time.time() - self.training_start_time if self.training_start_time else 0
        }
        
        with open(filepath, 'w') as f:
            json.dump(log_data, f, indent=2)
        
        print(f"ğŸ’¾ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {filepath}")
    
    def plot_training_progress(self, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å…ƒæŸå¤±æ›²çº¿
        if self.meta_losses:
            axes[0, 0].plot(self.meta_losses)
            axes[0, 0].set_title('å…ƒå­¦ä¹ æŸå¤±')
            axes[0, 0].set_xlabel('å…ƒå›åˆ')
            axes[0, 0].set_ylabel('æŸå¤±')
            axes[0, 0].grid(True)
        
        # é€‚åº”åˆ†æ•°
        if self.adaptation_scores:
            for task_type, scores in self.adaptation_scores.items():
                axes[0, 1].plot(scores, label=task_type, marker='o', markersize=3)
            axes[0, 1].set_title('å¿«é€Ÿé€‚åº”æ€§èƒ½')
            axes[0, 1].set_xlabel('è¯„ä¼°æ¬¡æ•°')
            axes[0, 1].set_ylabel('é€‚åº”åˆ†æ•°')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
        
        # è¿ç§»åˆ†æ•°
        if self.transfer_scores:
            transfer_means = [np.mean(scores) for scores in self.transfer_scores.values()]
            transfer_labels = list(self.transfer_scores.keys())
            axes[1, 0].bar(range(len(transfer_means)), transfer_means)
            axes[1, 0].set_title('è·¨åŸŸè¿ç§»æ€§èƒ½')
            axes[1, 0].set_xlabel('è¿ç§»æ–¹å‘')
            axes[1, 0].set_ylabel('è¿ç§»åˆ†æ•°')
            axes[1, 0].set_xticks(range(len(transfer_labels)))
            axes[1, 0].set_xticklabels(transfer_labels, rotation=45, ha='right')
            axes[1, 0].grid(True, axis='y')
        
        # è®­ç»ƒæ—¶é—´åˆ†å¸ƒ
        if self.episode_times:
            axes[1, 1].hist(self.episode_times, bins=20, alpha=0.7)
            axes[1, 1].set_title('å›åˆç”¨æ—¶åˆ†å¸ƒ')
            axes[1, 1].set_xlabel('æ—¶é—´ (ç§’)')
            axes[1, 1].set_ylabel('é¢‘æ¬¡')
            axes[1, 1].grid(True, axis='y')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
    
    def demonstrate_few_shot_learning(self, task_config: TaskConfig, 
                                    support_sizes: List[int] = [1, 5, 10, 20]):
        """
        æ¼”ç¤ºå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›
        
        Args:
            task_config: æµ‹è¯•ä»»åŠ¡é…ç½®
            support_sizes: ä¸åŒçš„æ”¯æŒé›†å¤§å°
        """
        print(f"ğŸ¯ æ¼”ç¤ºå°‘æ ·æœ¬å­¦ä¹ : {task_config.task_type.value}")
        
        env = MetaEnvironmentWrapper(self.base_env_class, task_config)
        results = {}
        
        for support_size in support_sizes:
            print(f"ğŸ“Š æµ‹è¯•æ”¯æŒé›†å¤§å°: {support_size}")
            
            # é‡ç½®æ™ºèƒ½ä½“åˆ°å…ƒç½‘ç»œçŠ¶æ€
            self.agent.task_network.load_state_dict(self.agent.meta_network.state_dict())
            self.agent.set_task(task_config)
            
            # æ”¶é›†æ”¯æŒæ•°æ®
            support_data = []
            for _ in range(support_size):
                state, _ = env.reset()
                action = self.agent.act(state, eps=0.3)
                next_state, reward, done, truncated, _ = env.step(action)
                support_data.append((state, action, reward, next_state, done or truncated))
            
            # å¿«é€Ÿé€‚åº”
            adaptation_losses = self.agent.fast_adapt(support_data)
            
            # æµ‹è¯•æ€§èƒ½
            test_scores = []
            for _ in range(10):
                state, _ = env.reset()
                total_reward = 0
                
                for step in range(self.config['max_steps_per_episode']):
                    action = self.agent.act(state, eps=0.0)
                    next_state, reward, done, truncated, _ = env.step(action)
                    total_reward += reward
                    state = next_state
                    
                    if done or truncated:
                        break
                
                test_scores.append(total_reward)
            
            avg_score = np.mean(test_scores)
            std_score = np.std(test_scores)
            
            results[support_size] = {
                'avg_score': avg_score,
                'std_score': std_score,
                'adaptation_losses': adaptation_losses
            }
            
            print(f"   å¹³å‡åˆ†æ•°: {avg_score:.2f} Â± {std_score:.2f}")
        
        return results

if __name__ == '__main__':
    # æµ‹è¯•å…ƒå­¦ä¹ è®­ç»ƒå™¨
    print("ğŸ§ª æµ‹è¯•å…ƒå­¦ä¹ è®­ç»ƒå™¨")
    
    # åˆ›å»ºç»„ä»¶
    task_generator = MetaTaskGenerator(seed=42)
    agent = MetaDQNAgent(state_size=8, action_size=5, seed=42)
    trainer = MetaTrainer(agent, task_generator)
    
    # æµ‹è¯•ä»»åŠ¡æ•°æ®æ”¶é›†
    test_task = task_generator.generate_task()
    task_data = trainer._collect_task_data(test_task)
    
    print(f"âœ… æ”¶é›†åˆ°æ”¯æŒé›†æ•°æ®: {len(task_data['support'])} ä¸ª")
    print(f"âœ… æ”¶é›†åˆ°æŸ¥è¯¢é›†æ•°æ®: {len(task_data['query'])} ä¸ª")
    
    print("âœ… å…ƒå­¦ä¹ è®­ç»ƒå™¨æµ‹è¯•å®Œæˆï¼")

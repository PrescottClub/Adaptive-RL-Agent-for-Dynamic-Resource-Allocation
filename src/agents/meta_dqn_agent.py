import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import copy
from collections import deque, OrderedDict
from typing import List, Dict, Tuple, Any

try:
    from ..models.dqn_model import DQN
    from ..utils.replay_buffer import ReplayBuffer
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    from pathlib import Path
    project_root = Path(__file__).parent.parent.parent
    sys.path.append(str(project_root))
    from src.models.dqn_model import DQN
    from src.utils.replay_buffer import ReplayBuffer

class MetaDQN(nn.Module):
    """
    å…ƒå­¦ä¹ DQNç½‘ç»œ - æ”¯æŒå¿«é€Ÿé€‚åº”çš„ç¥ç»ç½‘ç»œæ¶æ„
    """
    
    def __init__(self, n_observations, n_actions, hidden_size=128, meta_lr=1e-3):
        super(MetaDQN, self).__init__()
        self.n_observations = n_observations
        self.n_actions = n_actions
        self.hidden_size = hidden_size
        self.meta_lr = meta_lr
        
        # ä¸»ç½‘ç»œå±‚
        self.layer1 = nn.Linear(n_observations, hidden_size)
        self.layer2 = nn.Linear(hidden_size, hidden_size)
        self.layer3 = nn.Linear(hidden_size, n_actions)
        
        # å…ƒå­¦ä¹ ç‰¹å®šçš„é€‚åº”å±‚
        self.adaptation_layer = nn.Linear(hidden_size, hidden_size)
        
        # ä»»åŠ¡åµŒå…¥å±‚ - ç”¨äºç¼–ç ä»»åŠ¡ç‰¹å¾
        self.task_embedding = nn.Linear(10, hidden_size)  # ä»»åŠ¡ç‰¹å¾ç»´åº¦ä¸º10ï¼Œè¾“å‡ºä¸éšè—å±‚ç›¸åŒ
        
    def forward(self, x, task_features=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: çŠ¶æ€è§‚å¯Ÿ
            task_features: ä»»åŠ¡ç‰¹å¾å‘é‡
        """
        # åŸºç¡€ç‰¹å¾æå–
        h1 = F.relu(self.layer1(x))
        h2 = F.relu(self.layer2(h1))
        
        # å¦‚æœæœ‰ä»»åŠ¡ç‰¹å¾ï¼Œè¿›è¡Œä»»åŠ¡ç‰¹å®šçš„é€‚åº”
        if task_features is not None:
            # ç¡®ä¿ä»»åŠ¡ç‰¹å¾æ˜¯æ­£ç¡®çš„å½¢çŠ¶
            if task_features.dim() == 1:
                task_features = task_features.unsqueeze(0)

            # ç”Ÿæˆä»»åŠ¡åµŒå…¥
            task_emb = F.relu(self.task_embedding(task_features))

            # å°†ä»»åŠ¡åµŒå…¥æ‰©å±•åˆ°åŒ¹é…æ‰¹æ¬¡å¤§å°
            if task_emb.size(0) == 1 and h2.size(0) > 1:
                task_emb = task_emb.expand(h2.size(0), -1)

            # ç›´æ¥ç›¸åŠ ï¼ˆç°åœ¨ç»´åº¦åº”è¯¥åŒ¹é…ï¼‰
            h2_adapted = h2 + task_emb
        else:
            h2_adapted = h2
        
        # è¾“å‡ºQå€¼
        q_values = self.layer3(h2_adapted)
        return q_values
    
    def get_task_features(self, task_config):
        """
        ä»ä»»åŠ¡é…ç½®ä¸­æå–ç‰¹å¾å‘é‡
        
        Args:
            task_config: ä»»åŠ¡é…ç½®å¯¹è±¡
            
        Returns:
            torch.Tensor: ä»»åŠ¡ç‰¹å¾å‘é‡
        """
        # ç®€åŒ–çš„ä»»åŠ¡ç‰¹å¾æå–
        features = [
            task_config.resource_count / 10.0,  # å½’ä¸€åŒ–èµ„æºæ•°é‡
            task_config.difficulty_level,       # éš¾åº¦çº§åˆ«
            len(task_config.reward_weights),     # å¥–åŠ±ç»„ä»¶æ•°é‡
        ]
        
        # æ·»åŠ ä»»åŠ¡ç±»å‹çš„one-hotç¼–ç 
        task_type_encoding = [0.0] * 4
        task_types = ['network_traffic', 'cloud_computing', 'smart_grid', 'vehicle_routing']
        if task_config.task_type.value in task_types:
            idx = task_types.index(task_config.task_type.value)
            task_type_encoding[idx] = 1.0
        
        features.extend(task_type_encoding)
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(features) < 10:
            features.append(0.0)
        
        return torch.tensor(features[:10], dtype=torch.float32)

class MetaDQNAgent:
    """
    å…ƒå­¦ä¹ DQNæ™ºèƒ½ä½“ - å®ç°MAMLç®—æ³•çš„DQNç‰ˆæœ¬
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼ˆfew-shot learningï¼‰
    2. è·¨ä»»åŠ¡çŸ¥è¯†è¿ç§»
    3. å…ƒè®­ç»ƒå’Œå¿«é€Ÿå¾®è°ƒ
    """
    
    def __init__(self, state_size, action_size, lr=5e-4, meta_lr=1e-3, 
                 buffer_size=int(1e5), batch_size=64, gamma=0.99, 
                 tau=1e-3, update_every=4, adaptation_steps=5, seed=None):
        """
        åˆå§‹åŒ–å…ƒå­¦ä¹ DQNæ™ºèƒ½ä½“
        
        Args:
            adaptation_steps: å¿«é€Ÿé€‚åº”çš„æ¢¯åº¦æ­¥æ•°
        """
        self.state_size = state_size
        self.action_size = action_size
        self.lr = lr
        self.meta_lr = meta_lr
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau
        self.update_every = update_every
        self.adaptation_steps = adaptation_steps
        self.seed = seed
        
        # è®¾å¤‡é€‰æ‹©
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        
        # å…ƒç½‘ç»œï¼ˆç”¨äºå…ƒè®­ç»ƒï¼‰
        self.meta_network = MetaDQN(state_size, action_size, meta_lr=meta_lr).to(self.device)
        self.meta_optimizer = optim.Adam(self.meta_network.parameters(), lr=meta_lr)
        
        # å½“å‰ä»»åŠ¡ç½‘ç»œï¼ˆä»å…ƒç½‘ç»œå¤åˆ¶è€Œæ¥ï¼‰
        self.task_network = MetaDQN(state_size, action_size, meta_lr=meta_lr).to(self.device)
        self.task_optimizer = optim.Adam(self.task_network.parameters(), lr=lr)
        
        # ç›®æ ‡ç½‘ç»œ
        self.target_network = MetaDQN(state_size, action_size, meta_lr=meta_lr).to(self.device)
        
        # ç»éªŒå›æ”¾
        self.memory = ReplayBuffer(buffer_size, batch_size, seed, self.device)
        
        # å…ƒå­¦ä¹ ç›¸å…³
        self.current_task_config = None
        self.task_features = None
        self.meta_batch_size = 4  # å…ƒæ‰¹æ¬¡ä¸­çš„ä»»åŠ¡æ•°é‡
        
        # è®­ç»ƒç»Ÿè®¡
        self.t_step = 0
        self.adaptation_history = []
        
    def set_task(self, task_config):
        """
        è®¾ç½®å½“å‰ä»»åŠ¡å¹¶è¿›è¡Œå¿«é€Ÿé€‚åº”
        
        Args:
            task_config: ä»»åŠ¡é…ç½®
        """
        self.current_task_config = task_config
        self.task_features = self.meta_network.get_task_features(task_config)
        
        # æ£€æŸ¥ä»»åŠ¡ç»´åº¦ä¿¡æ¯ï¼ˆç°åœ¨ä½¿ç”¨ç»Ÿä¸€ç»´åº¦ï¼Œæ­¤æ£€æŸ¥ä¸»è¦ç”¨äºè°ƒè¯•ï¼‰
        expected_obs_dim = task_config.resource_count * 2
        expected_action_dim = task_config.resource_count * 2
        
        # ç°åœ¨æ‰€æœ‰ä»»åŠ¡éƒ½ä½¿ç”¨ç»Ÿä¸€çš„16ç»´çŠ¶æ€ç©ºé—´å’Œ16ç»´åŠ¨ä½œç©ºé—´
        # å®é™…çš„ä»»åŠ¡ç‰¹å®šç»´åº¦é€šè¿‡è§‚å¯Ÿç©ºé—´é€‚é…å™¨å¤„ç†
        
        # ä»å…ƒç½‘ç»œå¤åˆ¶å‚æ•°åˆ°ä»»åŠ¡ç½‘ç»œ
        self.task_network.load_state_dict(self.meta_network.state_dict())
        
        # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.task_optimizer = optim.Adam(self.task_network.parameters(), lr=self.lr)
        
        print(f"ğŸ¯ è®¾ç½®æ–°ä»»åŠ¡: {task_config.task_type.value} (éš¾åº¦: {task_config.difficulty_level:.2f})")
    
    def act(self, state, eps=0.0):
        """
        é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            eps: epsilonå€¼ï¼ˆç”¨äºepsilon-greedyç­–ç•¥ï¼‰
        """
        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)
        
        self.task_network.eval()
        with torch.no_grad():
            if self.task_features is not None:
                task_features_batch = self.task_features.unsqueeze(0).to(self.device)
                action_values = self.task_network(state, task_features_batch)
            else:
                action_values = self.task_network(state)
        self.task_network.train()
        
        # Epsilon-greedyåŠ¨ä½œé€‰æ‹©
        if np.random.random() > eps:
            return np.argmax(action_values.cpu().data.numpy())
        else:
            return np.random.choice(np.arange(self.action_size))
    
    def step(self, state, action, reward, next_state, done):
        """
        ä¿å­˜ç»éªŒå¹¶å­¦ä¹ 
        """
        # ä¿å­˜ç»éªŒ
        self.memory.add(state, action, reward, next_state, done)
        
        # å­¦ä¹ 
        self.t_step = (self.t_step + 1) % self.update_every
        if self.t_step == 0:
            if len(self.memory) > self.batch_size:
                experiences = self.memory.sample()
                self.learn(experiences, self.gamma)
    
    def learn(self, experiences, gamma):
        """
        ä»ç»éªŒä¸­å­¦ä¹ ï¼ˆä»»åŠ¡ç‰¹å®šçš„å­¦ä¹ ï¼‰
        """
        states, actions, rewards, next_states, dones = experiences
        
        # å‡†å¤‡ä»»åŠ¡ç‰¹å¾
        if self.task_features is not None:
            batch_size = states.size(0)
            task_features_batch = self.task_features.unsqueeze(0).expand(batch_size, -1).to(self.device)
        else:
            task_features_batch = None
        
        # è®¡ç®—å½“å‰Qå€¼
        Q_expected = self.task_network(states, task_features_batch).gather(1, actions)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        Q_targets_next = self.target_network(next_states, task_features_batch).detach().max(1)[0].unsqueeze(1)
        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(Q_expected, Q_targets)
        
        # ä¼˜åŒ–
        self.task_optimizer.zero_grad()
        loss.backward()
        self.task_optimizer.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.soft_update(self.task_network, self.target_network, self.tau)
    
    def fast_adapt(self, support_data: List[Tuple], adaptation_lr=None):
        """
        å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼ˆMAMLçš„å†…å¾ªç¯ï¼‰
        
        Args:
            support_data: æ”¯æŒé›†æ•°æ® [(state, action, reward, next_state, done), ...]
            adaptation_lr: é€‚åº”å­¦ä¹ ç‡
        """
        if adaptation_lr is None:
            adaptation_lr = self.lr
        
        # åˆ›å»ºä¸´æ—¶ç½‘ç»œç”¨äºé€‚åº”
        adapted_network = copy.deepcopy(self.task_network)
        adapted_optimizer = optim.SGD(adapted_network.parameters(), lr=adaptation_lr)
        
        adaptation_losses = []
        
        # æ‰§è¡Œå‡ æ­¥æ¢¯åº¦ä¸‹é™
        for step in range(self.adaptation_steps):
            total_loss = 0.0
            
            for state, action, reward, next_state, done in support_data:
                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)
                next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0).to(self.device)
                action_tensor = torch.tensor([[action]], dtype=torch.long).to(self.device)
                reward_tensor = torch.tensor([[reward]], dtype=torch.float).to(self.device)
                done_tensor = torch.tensor([[done]], dtype=torch.float).to(self.device)
                
                # å‡†å¤‡ä»»åŠ¡ç‰¹å¾
                if self.task_features is not None:
                    task_features_batch = self.task_features.unsqueeze(0).to(self.device)
                else:
                    task_features_batch = None
                
                # è®¡ç®—Qå€¼å’ŒæŸå¤±
                Q_current = adapted_network(state_tensor, task_features_batch).gather(1, action_tensor)
                Q_next = adapted_network(next_state_tensor, task_features_batch).detach().max(1)[0].unsqueeze(1)
                Q_target = reward_tensor + (self.gamma * Q_next * (1 - done_tensor))
                
                loss = F.mse_loss(Q_current, Q_target)
                total_loss += loss
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            adapted_optimizer.zero_grad()
            total_loss.backward()
            adapted_optimizer.step()
            
            adaptation_losses.append(total_loss.item())
        
        # æ›´æ–°ä»»åŠ¡ç½‘ç»œ
        self.task_network.load_state_dict(adapted_network.state_dict())
        self.adaptation_history.append(adaptation_losses)
        
        print(f"ğŸ”„ å¿«é€Ÿé€‚åº”å®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {adaptation_losses[-1]:.4f}")
        
        return adaptation_losses
    
    def meta_update(self, meta_batch_data: List[Dict]):
        """
        å…ƒæ›´æ–°ï¼ˆMAMLçš„å¤–å¾ªç¯ï¼‰
        
        Args:
            meta_batch_data: å…ƒæ‰¹æ¬¡æ•°æ®ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ä¸€ä¸ªä»»åŠ¡çš„æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†
        """
        meta_loss = 0.0
        
        for task_data in meta_batch_data:
            support_set = task_data['support']
            query_set = task_data['query']
            task_config = task_data['task_config']
            
            # è®¾ç½®ä»»åŠ¡
            old_task_config = self.current_task_config
            self.set_task(task_config)
            
            # åœ¨æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚åº”
            self.fast_adapt(support_set)
            
            # åœ¨æŸ¥è¯¢é›†ä¸Šè®¡ç®—æŸå¤±
            query_loss = self._compute_query_loss(query_set)
            meta_loss += query_loss
            
            # æ¢å¤åŸä»»åŠ¡é…ç½®
            self.current_task_config = old_task_config
        
        # å…ƒä¼˜åŒ–
        meta_loss = meta_loss / len(meta_batch_data)
        self.meta_optimizer.zero_grad()
        meta_loss.backward()
        self.meta_optimizer.step()
        
        print(f"ğŸŒŸ å…ƒæ›´æ–°å®Œæˆï¼Œå…ƒæŸå¤±: {meta_loss.item():.4f}")
        
        return meta_loss.item()
    
    def _compute_query_loss(self, query_data: List[Tuple]):
        """è®¡ç®—æŸ¥è¯¢é›†ä¸Šçš„æŸå¤±"""
        total_loss = 0.0
        
        for state, action, reward, next_state, done in query_data:
            state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)
            next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0).to(self.device)
            action_tensor = torch.tensor([[action]], dtype=torch.long).to(self.device)
            reward_tensor = torch.tensor([[reward]], dtype=torch.float).to(self.device)
            done_tensor = torch.tensor([[done]], dtype=torch.float).to(self.device)
            
            # å‡†å¤‡ä»»åŠ¡ç‰¹å¾
            if self.task_features is not None:
                task_features_batch = self.task_features.unsqueeze(0).to(self.device)
            else:
                task_features_batch = None
            
            # è®¡ç®—Qå€¼å’ŒæŸå¤±
            Q_current = self.task_network(state_tensor, task_features_batch).gather(1, action_tensor)
            Q_next = self.task_network(next_state_tensor, task_features_batch).detach().max(1)[0].unsqueeze(1)
            Q_target = reward_tensor + (self.gamma * Q_next * (1 - done_tensor))
            
            loss = F.mse_loss(Q_current, Q_target)
            total_loss += loss
        
        return total_loss
    
    def soft_update(self, local_model, target_model, tau):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°"""
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)
    
    def save(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'meta_network_state_dict': self.meta_network.state_dict(),
            'task_network_state_dict': self.task_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'meta_optimizer_state_dict': self.meta_optimizer.state_dict(),
            'task_optimizer_state_dict': self.task_optimizer.state_dict(),
            'adaptation_history': self.adaptation_history
        }, filepath)
    
    def load(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.meta_network.load_state_dict(checkpoint['meta_network_state_dict'])
        self.task_network.load_state_dict(checkpoint['task_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.meta_optimizer.load_state_dict(checkpoint['meta_optimizer_state_dict'])
        self.task_optimizer.load_state_dict(checkpoint['task_optimizer_state_dict'])
        self.adaptation_history = checkpoint.get('adaptation_history', [])

if __name__ == '__main__':
    # æµ‹è¯•å…ƒå­¦ä¹ DQNæ™ºèƒ½ä½“
    print("ğŸ§ª æµ‹è¯•å…ƒå­¦ä¹ DQNæ™ºèƒ½ä½“")
    
    agent = MetaDQNAgent(state_size=8, action_size=5, seed=42)
    
    # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
    dummy_state = np.random.rand(8)
    action = agent.act(dummy_state, eps=0.1)
    print(f"ğŸ¯ é€‰æ‹©çš„åŠ¨ä½œ: {action}")
    
    # æµ‹è¯•ä»»åŠ¡è®¾ç½®
    try:
        from ..environments.meta_task_generator import MetaTaskGenerator, TaskType
    except ImportError:
        from src.environments.meta_task_generator import MetaTaskGenerator, TaskType

    generator = MetaTaskGenerator(seed=42)
    task_config = generator.generate_task(TaskType.NETWORK_TRAFFIC)
    agent.set_task(task_config)
    
    print("âœ… å…ƒå­¦ä¹ DQNæ™ºèƒ½ä½“æµ‹è¯•å®Œæˆï¼")
